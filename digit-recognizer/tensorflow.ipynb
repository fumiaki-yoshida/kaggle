{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('train.csv')\n",
    "dataset_t = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x27623225da0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#show image\n",
    "some_digit = dataset.iloc[100,1:]\n",
    "some_digit_image = some_digit.values.reshape(28,28) # reshape to square\n",
    "plt.imshow(some_digit_image,cmap = plt.cm.binary,interpolation=\"nearest\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fumiaki\\Anaconda3\\envs\\tf_conda\\lib\\site-packages\\sklearn\\utils\\validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "#Encode categorical integer features using a one-hot \n",
    "dataset=pd.get_dummies(dataset,columns=[\"label\"])\n",
    "\n",
    "# Seprate X and y\n",
    "dataset_X = dataset.iloc[:, :-10].values\n",
    "dataset_y = dataset.iloc[:, -10:].values\n",
    "dataset_t=dataset_t.values\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "dataset_X = sc.fit_transform(dataset_X)\n",
    "dataset_t = sc.transform(dataset_t)\n",
    "\n",
    "\n",
    "#Split trainning and testing caseï¼Œmake sure every digit have equal chance in both testing and trainning set.\n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset_X, dataset_y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=dataset_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "dataset_X = sc.fit_transform(dataset_X)\n",
    "dataset_t = sc.transform(dataset_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(dataset_X, dataset_y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    stratify=dataset_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(33600, 784)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-8-fdf3bb037297>:21: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "g1 = tf.Graph()\n",
    "\n",
    "#Create g1 Graph\n",
    "g1 = tf.Graph()\n",
    "#Set g1 Graph as default graph, and construct it.\n",
    "with g1.as_default():\n",
    "    X = tf.placeholder(tf.float32,[None,784]) #for input \n",
    "    \n",
    "    W= tf.Variable(tf.zeros([784,10]))\n",
    "    b= tf.Variable(tf.zeros([10]))\n",
    "\n",
    "    #model\n",
    "    Ylogits=tf.matmul(X,W) + b\n",
    "    # If you want to assign probabilities to an object being one of several different things,\n",
    "    #softmax is the thing to do, because softmax gives us a list of values between 0 and 1 that add up to 1. \n",
    "    Y = tf.nn.softmax(Ylogits)\n",
    "    #placeholder for correct labels\n",
    "    Y_=tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "    # loss function\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits,labels=Y_))\n",
    "    \n",
    "    #Evaluate the model\n",
    "    correct_prediction = tf.equal(tf.argmax(Y,1), tf.argmax(Y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5)#0.5 is \n",
    "    train_step = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runmodel():\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    batch_size=100 # each time how many case input to NN for trainning\n",
    "    epoch=20 # how many time the NN view the whole data set\n",
    "    iterations=int(x_train.shape[0]/batch_size)\n",
    "    \n",
    "    batchnumber=0\n",
    "   # mini batch\n",
    "    for e in range(epoch):\n",
    "        for i in range(iterations):\n",
    "            batchnumber= batchnumber+1\n",
    "            batch_start_idx = (i * batch_size) % (x_train.shape[0] - batch_size)\n",
    "            batch_end_idx = batch_start_idx + batch_size\n",
    "            batch_X = x_train[batch_start_idx:batch_end_idx]\n",
    "            batch_Y = y_train[batch_start_idx:batch_end_idx]\n",
    "            train_data = {X:batch_X,Y_:batch_Y}\n",
    "            # train\n",
    "            sess.run(train_step, feed_dict=train_data)\n",
    "        print (\"Epoch\"+str(e+1))\n",
    "        print (\"batch: \"+ str(batchnumber+1))\n",
    "        ans=sess.run(accuracy,feed_dict={X:x_test,Y_:y_test}) # evaluate the testing dataset.\n",
    "        print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1\n",
      "batch: 337\n",
      "0.8977381\n",
      "Epoch2\n",
      "batch: 673\n",
      "0.8989286\n",
      "Epoch3\n",
      "batch: 1009\n",
      "0.89678574\n",
      "Epoch4\n",
      "batch: 1345\n",
      "0.8971428\n",
      "Epoch5\n",
      "batch: 1681\n",
      "0.8970238\n",
      "Epoch6\n",
      "batch: 2017\n",
      "0.8982143\n",
      "Epoch7\n",
      "batch: 2353\n",
      "0.89940476\n",
      "Epoch8\n",
      "batch: 2689\n",
      "0.89988095\n",
      "Epoch9\n",
      "batch: 3025\n",
      "0.8978571\n",
      "Epoch10\n",
      "batch: 3361\n",
      "0.8996429\n",
      "Epoch11\n",
      "batch: 3697\n",
      "0.89761907\n",
      "Epoch12\n",
      "batch: 4033\n",
      "0.89690477\n",
      "Epoch13\n",
      "batch: 4369\n",
      "0.8979762\n",
      "Epoch14\n",
      "batch: 4705\n",
      "0.89607143\n",
      "Epoch15\n",
      "batch: 5041\n",
      "0.89607143\n",
      "Epoch16\n",
      "batch: 5377\n",
      "0.89678574\n",
      "Epoch17\n",
      "batch: 5713\n",
      "0.89547616\n",
      "Epoch18\n",
      "batch: 6049\n",
      "0.8934524\n",
      "Epoch19\n",
      "batch: 6385\n",
      "0.89619046\n",
      "Epoch20\n",
      "batch: 6721\n",
      "0.8947619\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph = g1) as sess:\n",
    "    runmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "g2 = tf.Graph()\n",
    "with g2.as_default():\n",
    "    X = tf.placeholder(tf.float32,[None,784])\n",
    "    \n",
    "    # Hidden 1\n",
    "    with tf.name_scope('hidden1'):\n",
    "        weights = tf.Variable(\n",
    "        tf.truncated_normal([784, 120],\n",
    "                            stddev=1.0 / math.sqrt(float(784))),name='weights')\n",
    "        biases = tf.Variable(tf.zeros([120]),\n",
    "                         name='biases')\n",
    "        hidden1 = tf.nn.relu(tf.matmul(X, weights) + biases)\n",
    "    \n",
    "    # Hidden 2\n",
    "    with tf.name_scope('hidden2'):\n",
    "        weights = tf.Variable(\n",
    "        tf.truncated_normal([120, 32],\n",
    "                            stddev=1.0 / math.sqrt(float(128))),\n",
    "        name='weights')\n",
    "        biases = tf.Variable(tf.zeros([32]),\n",
    "                         name='biases')\n",
    "        hidden2 = tf.nn.relu(tf.matmul(hidden1, weights) + biases)\n",
    "  # Linear    \n",
    "    with tf.name_scope('softmax_linear'):\n",
    "        weights = tf.Variable(tf.zeros([32,10]),name='weights')\n",
    "        biases = tf.Variable(tf.zeros([10]),name='biases')\n",
    "        Ylogits = tf.matmul(hidden2, weights) + biases\n",
    "        Y = tf.nn.softmax(Ylogits)\n",
    "    \n",
    "    Y_=tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "    # loss function\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits,labels=Y_))\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(Y,1), tf.argmax(Y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.005) #learnning rate is key\n",
    "    train_step = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1\n",
      "batch: 337\n",
      "0.3895238\n",
      "Epoch2\n",
      "batch: 673\n",
      "0.4097619\n",
      "Epoch3\n",
      "batch: 1009\n",
      "0.47928572\n",
      "Epoch4\n",
      "batch: 1345\n",
      "0.67511904\n",
      "Epoch5\n",
      "batch: 1681\n",
      "0.8025\n",
      "Epoch6\n",
      "batch: 2017\n",
      "0.8472619\n",
      "Epoch7\n",
      "batch: 2353\n",
      "0.8736905\n",
      "Epoch8\n",
      "batch: 2689\n",
      "0.8863095\n",
      "Epoch9\n",
      "batch: 3025\n",
      "0.89285713\n",
      "Epoch10\n",
      "batch: 3361\n",
      "0.8997619\n",
      "Epoch11\n",
      "batch: 3697\n",
      "0.9054762\n",
      "Epoch12\n",
      "batch: 4033\n",
      "0.91083336\n",
      "Epoch13\n",
      "batch: 4369\n",
      "0.9159524\n",
      "Epoch14\n",
      "batch: 4705\n",
      "0.91880953\n",
      "Epoch15\n",
      "batch: 5041\n",
      "0.9210714\n",
      "Epoch16\n",
      "batch: 5377\n",
      "0.92392856\n",
      "Epoch17\n",
      "batch: 5713\n",
      "0.9260714\n",
      "Epoch18\n",
      "batch: 6049\n",
      "0.92833334\n",
      "Epoch19\n",
      "batch: 6385\n",
      "0.93083334\n",
      "Epoch20\n",
      "batch: 6721\n",
      "0.9322619\n"
     ]
    }
   ],
   "source": [
    "# run the second ANN\n",
    "with tf.Session(graph = g2) as sess:\n",
    "    runmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "g3 = tf.Graph()\n",
    "with g3.as_default():\n",
    "    X = tf.placeholder(tf.float32,[None,784])\n",
    "    X_image = tf.reshape(X,[-1,28,28,1])\n",
    "    \n",
    "    # correct answers will go here\n",
    "    Y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    K = 32  # first convolutional layer output depth\n",
    "    L = 64  # second convolutional layer output depth\n",
    "    N = 1024  # fully connected layer\n",
    "    \n",
    "    W1 = tf.Variable(tf.truncated_normal([5, 5, 1, K], stddev=0.1))  # 5x5 patch, 1 input channel, K output channels\n",
    "    B1 = tf.Variable(tf.ones([K])/10)\n",
    "    \n",
    "    W2 = tf.Variable(tf.truncated_normal([5, 5, K, L], stddev=0.1))\n",
    "    B2 = tf.Variable(tf.ones([L])/10)\n",
    "    \n",
    "    W3 = tf.Variable(tf.truncated_normal([7 * 7 * L, N], stddev=0.1))\n",
    "    B3 = tf.Variable(tf.ones([N])/10)\n",
    "    \n",
    "    W4 = tf.Variable(tf.truncated_normal([N, 10], stddev=0.1))\n",
    "    B4 = tf.Variable(tf.ones([10])/10)\n",
    "    \n",
    "    # The model\n",
    "    \n",
    "    stride = 1  # output is 28x28*32\n",
    "    Y1 = tf.nn.relu(tf.nn.conv2d(X_image, W1, strides=[1, stride, stride, 1], padding='SAME') + B1)\n",
    "    \n",
    "    #Max pooling, output is 14*14*32\n",
    "    Y2 = tf.nn.max_pool(Y1,ksize=[1,2,2,1],strides=[1,2,2,1], padding='SAME')\n",
    "    \n",
    "    stride = 1  # output is 14x14*64\n",
    "    Y3 = tf.nn.relu(tf.nn.conv2d(Y2, W2, strides=[1, stride, stride, 1], padding='SAME') + B2)\n",
    "    \n",
    "    #Max pooling, output is 7*7*64\n",
    "    Y4 = tf.nn.max_pool(Y3,ksize=[1,2,2,1],strides=[1,2,2,1], padding='SAME')\n",
    "    \n",
    "    # reshape the output from the second Max pooling for the fully connected layer\n",
    "    YY = tf.reshape(Y4, shape=[-1, 7 * 7 * L])\n",
    "    \n",
    "    Y5 = tf.nn.relu(tf.matmul(YY, W3) + B3)\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    full1_drop = tf.nn.dropout(Y5, keep_prob=keep_prob)\n",
    "    \n",
    "    Ylogits = tf.matmul(full1_drop, W4) + B4\n",
    "    Y = tf.nn.softmax(Ylogits)\n",
    "    \n",
    "     # loss function\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=Ylogits,labels=Y_))\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(Y,1), tf.argmax(Y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # variable learning rate\n",
    "    lr = tf.placeholder(tf.float32)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(lr)\n",
    "    train_step = optimizer.minimize(cross_entropy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1\n",
      "batch: 673\n",
      "0.9522619\n",
      "Epoch2\n",
      "batch: 1345\n",
      "0.97011906\n",
      "Epoch3\n",
      "batch: 2017\n",
      "0.9696429\n",
      "Epoch4\n",
      "batch: 2689\n",
      "0.97321427\n",
      "Epoch5\n",
      "batch: 3361\n",
      "0.9697619\n",
      "Epoch6\n",
      "batch: 4033\n",
      "0.97261906\n",
      "Epoch7\n",
      "batch: 4705\n",
      "0.9745238\n",
      "Epoch8\n",
      "batch: 5377\n",
      "0.9767857\n",
      "Epoch9\n",
      "batch: 6049\n",
      "0.97952384\n",
      "Epoch10\n",
      "batch: 6721\n",
      "0.9765476\n"
     ]
    }
   ],
   "source": [
    "with tf.Session( graph = g3) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    batch_size=50 # each time how many case input to NN for trainning\n",
    "    epoch=10 # how many time the NN view the whole data set\n",
    "    iterations=int(x_train.shape[0]/batch_size)\n",
    "    \n",
    "    batchnumber=0\n",
    "    \n",
    "    # learning rate decay\n",
    "    max_learning_rate = 0.004\n",
    "    min_learning_rate = 0.0001\n",
    "    decay_speed = 2000.0\n",
    "    learning_rate = min_learning_rate + (max_learning_rate - min_learning_rate) * math.exp(-batchnumber/decay_speed)\n",
    "    \n",
    "   # mini batch\n",
    "    for e in range(epoch):\n",
    "        for i in range(iterations):\n",
    "            batchnumber= batchnumber+1\n",
    "            batch_start_idx = (i * batch_size) % (x_train.shape[0] - batch_size)\n",
    "            batch_end_idx = batch_start_idx + batch_size\n",
    "            batch_X = x_train[batch_start_idx:batch_end_idx]\n",
    "            batch_Y = y_train[batch_start_idx:batch_end_idx]\n",
    "            train_data = {X:batch_X,Y_:batch_Y,keep_prob:0.5,lr:learning_rate} #50% node drop out\n",
    "            # train\n",
    "            sess.run(train_step, feed_dict=train_data)\n",
    "        print (\"Epoch\"+str(e+1))\n",
    "        print (\"batch: \"+ str(batchnumber+1))\n",
    "        ans=sess.run(accuracy,feed_dict={X:x_test,Y_:y_test,keep_prob:1,lr:learning_rate}) # evaluate the testing dataset.\n",
    "        print(ans)\n",
    "        \n",
    "     #Output the result\n",
    "    results = sess.run(Y,feed_dict={X:dataset_t,keep_prob:1})\n",
    "       \n",
    "     # select the indix with the maximum probability\n",
    "    results = np.argmax(results,axis = 1)\n",
    "\n",
    "    results = pd.Series(results,name=\"Label\")\n",
    "        \n",
    "    submission = pd.concat([pd.Series(range(1,28001),name = \"ImageId\"),results],axis = 1)\n",
    "\n",
    "    submission.to_csv(\"cnn_mnist_datagen.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "g4 = tf.Graph()\n",
    "with g4.as_default():\n",
    "    element_size = 28\n",
    "    time_steps = 28\n",
    "    num_classes =10\n",
    "    batch_size = 128\n",
    "    hidden_layer_size=128\n",
    "    \n",
    "    X = tf.placeholder(tf.float32,[None,784])\n",
    "    X_image = tf.reshape(X,[-1,28,28])\n",
    "    Y_ = tf.placeholder(tf.float32, shape=[None,num_classes])\n",
    "    \n",
    "    rnn_cell =tf.contrib.rnn.BasicRNNCell(hidden_layer_size)\n",
    "    outputs, _ = tf.nn.dynamic_rnn(rnn_cell,X_image,dtype=tf.float32)\n",
    "    \n",
    "    wl = tf.Variable(tf.truncated_normal([hidden_layer_size,num_classes],mean=0,stddev=.01))\n",
    "    bl = tf.Variable(tf.truncated_normal([num_classes],mean=0,stddev=.01))\n",
    "    \n",
    "    last_run_output = outputs[:,-1,:]\n",
    "    \n",
    "    final_output = tf.matmul(last_run_output,wl) + bl\n",
    "    \n",
    "     # loss function\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=final_output,labels=Y_))\n",
    "\n",
    "    correct_prediction = tf.equal(tf.argmax(final_output,1), tf.argmax(Y_,1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "    \n",
    "    optimizer = tf.train.RMSPropOptimizer(0.001,0.9)\n",
    "    train_step = optimizer.minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1\n",
      "batch: 337\n",
      "0.79452384\n",
      "Epoch2\n",
      "batch: 673\n",
      "0.88809526\n",
      "Epoch3\n",
      "batch: 1009\n",
      "0.91380954\n",
      "Epoch4\n",
      "batch: 1345\n",
      "0.92654765\n",
      "Epoch5\n",
      "batch: 1681\n",
      "0.942619\n",
      "Epoch6\n",
      "batch: 2017\n",
      "0.93738097\n",
      "Epoch7\n",
      "batch: 2353\n",
      "0.94416666\n",
      "Epoch8\n",
      "batch: 2689\n",
      "0.9507143\n",
      "Epoch9\n",
      "batch: 3025\n",
      "0.9508333\n",
      "Epoch10\n",
      "batch: 3361\n",
      "0.95297617\n",
      "Epoch11\n",
      "batch: 3697\n",
      "0.95738095\n",
      "Epoch12\n",
      "batch: 4033\n",
      "0.95607144\n",
      "Epoch13\n",
      "batch: 4369\n",
      "0.9388095\n",
      "Epoch14\n",
      "batch: 4705\n",
      "0.95714283\n",
      "Epoch15\n",
      "batch: 5041\n",
      "0.95357144\n",
      "Epoch16\n",
      "batch: 5377\n",
      "0.95535713\n",
      "Epoch17\n",
      "batch: 5713\n",
      "0.9584524\n",
      "Epoch18\n",
      "batch: 6049\n",
      "0.9622619\n",
      "Epoch19\n",
      "batch: 6385\n",
      "0.95238096\n",
      "Epoch20\n",
      "batch: 6721\n",
      "0.9564286\n"
     ]
    }
   ],
   "source": [
    "with tf.Session( graph = g4) as sess:\n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    batch_size=100 # each time how many case input to NN for trainning\n",
    "    epoch=20 # how many time the NN view the whole data set\n",
    "    iterations=int(x_train.shape[0]/batch_size)\n",
    "    \n",
    "    batchnumber=0\n",
    "   # mini batch\n",
    "    for e in range(epoch):\n",
    "        for i in range(iterations):\n",
    "            batchnumber= batchnumber+1\n",
    "            batch_start_idx = (i * batch_size) % (x_train.shape[0] - batch_size)\n",
    "            batch_end_idx = batch_start_idx + batch_size\n",
    "            batch_X = x_train[batch_start_idx:batch_end_idx]\n",
    "            batch_Y = y_train[batch_start_idx:batch_end_idx]\n",
    "            train_data = {X:batch_X,Y_:batch_Y}\n",
    "            # train\n",
    "            sess.run(train_step, feed_dict=train_data)\n",
    "        print (\"Epoch\"+str(e+1))\n",
    "        print (\"batch: \"+ str(batchnumber+1))\n",
    "        ans=sess.run(accuracy,feed_dict={X:x_test,Y_:y_test}) # evaluate the testing dataset.\n",
    "        print(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 2917437200280976042]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
